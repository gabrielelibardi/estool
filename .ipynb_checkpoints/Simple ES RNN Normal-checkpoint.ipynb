{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cma\n",
    "from es import SimpleGA, CMAES, PEPG, OpenES\n",
    "\n",
    "def sigmoid(x):\n",
    "    z = 1/(1 + np.exp(-x)) \n",
    "    return z\n",
    "\n",
    "def BCE_loss(y,p):\n",
    "    return np.mean(-y*np.log(p) - (1 - y)*np.log(1 - p))\n",
    "\n",
    "\n",
    "def binarize(x):\n",
    "    res = x > 0.5\n",
    "    return res.astype(int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(x, 0)\n",
    "\n",
    "def passthru(x):\n",
    "  return x\n",
    "\n",
    "# useful for discrete actions\n",
    "def softmax(x):\n",
    "  e_x = np.exp(x - np.max(x))\n",
    "  return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# useful for discrete actions\n",
    "def sample(p):\n",
    "  return np.argmax(np.random.multinomial(1, p))\n",
    "\n",
    "\n",
    "class RNNCell:\n",
    "  def __init__(self, input_size, weight, bias):\n",
    "    self.input_size=input_size\n",
    "    self.weight = weight\n",
    "    self.bias = bias\n",
    "  def __call__(self, x, h):\n",
    "    concat = np.concatenate((x, h), axis=1)\n",
    "    hidden = np.matmul(concat, self.weight)+self.bias\n",
    "    return np.tanh(hidden)\n",
    "\n",
    "class RNNModel:\n",
    "    def __init__(self):\n",
    "    \n",
    "\n",
    "        self.hidden_size = 10\n",
    "\n",
    "        self.layer_1 = 10\n",
    "        self.layer_2 = 10\n",
    "\n",
    "        self.rnn_mode = True\n",
    "\n",
    "        self.input_size = 1\n",
    "        self.output_size = 1\n",
    "        self.alpha = 0.0\n",
    "\n",
    "\n",
    "        self.shapes = [ (self.input_size + self.hidden_size, 1*self.hidden_size), # RNN weights\n",
    "                        (self.input_size + self.hidden_size, self.layer_1),# predict actions output\n",
    "                        (self.layer_1, self.output_size)] # predict actions output\n",
    "\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "        self.param_count = 0\n",
    "\n",
    "        idx = 0\n",
    "        for shape in self.shapes:\n",
    "          self.weight.append(np.zeros(shape=shape))\n",
    "          self.bias.append(np.zeros(shape=shape[1]))\n",
    "          self.param_count += (np.product(shape) + shape[1])\n",
    "          idx += 1\n",
    "\n",
    "        self.init_h = np.zeros((1, self.hidden_size))\n",
    "        self.h = self.init_h\n",
    "        self.param_count += 1*self.hidden_size\n",
    "\n",
    "        self.rnn = RNNCell(self.input_size, self.weight[0], self.bias[0])\n",
    "\n",
    "    def reset(self):\n",
    "        self.h = sigmoid(self.init_h)\n",
    "\n",
    "\n",
    "    def get_action(self, x):\n",
    "        obs = x.reshape(1, self.input_size)\n",
    "\n",
    "        # update rnn:\n",
    "        #update_obs = np.concatenate([obs, action], axis=1)\n",
    "       \n",
    "        self.h = sigmoid(self.rnn(x, self.h))\n",
    "\n",
    "        \n",
    "        # get action\n",
    "        x = np.concatenate([x, self.h], axis=1)\n",
    "\n",
    "        # calculate action using 2 layer network from output\n",
    "        hidden = np.tanh(np.matmul(x, self.weight[1]) + self.bias[1])\n",
    "        action = sigmoid(np.matmul(hidden, self.weight[2]) + self.bias[2])\n",
    "\n",
    "        return action[0]\n",
    "\n",
    "    def set_model_params(self, model_params):\n",
    "        pointer = 0\n",
    "        for i in range(len(self.shapes)):\n",
    "          w_shape = self.shapes[i]\n",
    "          b_shape = self.shapes[i][1]\n",
    "          s_w = np.product(w_shape)\n",
    "          s = s_w + b_shape\n",
    "          chunk = np.array(model_params[pointer:pointer+s])\n",
    "          self.weight[i] = chunk[:s_w].reshape(w_shape)\n",
    "          self.bias[i] = chunk[s_w:].reshape(b_shape)\n",
    "          pointer += s\n",
    "        # rnn states\n",
    "        s = self.hidden_size\n",
    "        self.init_h = model_params[pointer:pointer+s].reshape((1, self.hidden_size))\n",
    "        self.h = self.init_h\n",
    "        self.rnn = RNNCell(self.input_size, self.weight[0], self.bias[0])\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        with open(filename) as f:    \n",
    "          data = json.load(f)\n",
    "        print('loading file %s' % (filename))\n",
    "        self.data = data\n",
    "        model_params = np.array(data[0]) # assuming other stuff is in data\n",
    "        self.set_model_params(model_params)\n",
    "\n",
    "    def get_random_model_params(self, stdev=0.1):\n",
    "        return np.random.randn(self.param_count)*stdev\n",
    "\n",
    "    def update_alpha(self):\n",
    "        self.alpha += 0.001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RNNModel2:\n",
    "    def __init__(self):\n",
    "    \n",
    "\n",
    "        self.hidden_size = 10\n",
    "\n",
    "        self.layer_1 = 10\n",
    "        self.layer_2 = 10\n",
    "\n",
    "        self.rnn_mode = True\n",
    "\n",
    "        self.input_size = 1\n",
    "        self.output_size = 10\n",
    "        self.alpha = 1.0\n",
    "\n",
    "\n",
    "        self.shapes = [ (self.input_size + self.hidden_size, 1*self.hidden_size), # RNN weights\n",
    "                        (self.input_size + self.hidden_size, self.layer_1),# predict actions output\n",
    "                        (self.layer_1, self.output_size)] # predict actions output\n",
    "\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "        self.param_count = 0\n",
    "\n",
    "        idx = 0\n",
    "        for shape in self.shapes:\n",
    "          self.weight.append(np.zeros(shape=shape))\n",
    "          self.bias.append(np.zeros(shape=shape[1]))\n",
    "          self.param_count += (np.product(shape) + shape[1])\n",
    "          idx += 1\n",
    "\n",
    "        self.init_h = np.zeros((1, self.hidden_size))\n",
    "        self.h = self.init_h\n",
    "        self.param_count += 1*self.hidden_size\n",
    "\n",
    "        self.rnn = RNNCell(self.input_size, self.weight[0], self.bias[0])\n",
    "\n",
    "    def reset(self):\n",
    "        self.h = self.init_h\n",
    "        self.h =  (1 - self.alpha)*sigmoid(self.h)+ self.alpha*binarize(sigmoid(self.h))\n",
    "\n",
    "\n",
    "    def get_action(self, x):\n",
    "        obs = x.reshape(1, self.input_size)\n",
    "\n",
    "        # update rnn:\n",
    "        #update_obs = np.concatenate([obs, action], axis=1)\n",
    "        h =  (1 - self.alpha)*sigmoid(self.h)+ self.alpha*binarize(sigmoid(self.h))\n",
    "        self.h = self.rnn(x, h)\n",
    "\n",
    "        h =  (1 - self.alpha)*sigmoid(self.h)+ self.alpha*binarize(sigmoid(self.h))\n",
    "        # get action\n",
    "        x = np.concatenate([x, h], axis=1)\n",
    "\n",
    "        # calculate action using 2 layer network from output\n",
    "        hidden = np.tanh(np.matmul(x, self.weight[1]) + self.bias[1])\n",
    "        action = sigmoid(np.matmul(hidden, self.weight[2]) + self.bias[2])\n",
    "\n",
    "        return action[0]\n",
    "\n",
    "    def set_model_params(self, model_params):\n",
    "        pointer = 0\n",
    "        for i in range(len(self.shapes)):\n",
    "          w_shape = self.shapes[i]\n",
    "          b_shape = self.shapes[i][1]\n",
    "          s_w = np.product(w_shape)\n",
    "          s = s_w + b_shape\n",
    "          chunk = np.array(model_params[pointer:pointer+s])\n",
    "          self.weight[i] = chunk[:s_w].reshape(w_shape)\n",
    "          self.bias[i] = chunk[s_w:].reshape(b_shape)\n",
    "          pointer += s\n",
    "        # rnn states\n",
    "        s = self.hidden_size\n",
    "        self.init_h = model_params[pointer:pointer+s].reshape((1, self.hidden_size))\n",
    "        self.h = self.init_h\n",
    "        self.rnn = RNNCell(self.input_size, self.weight[0], self.bias[0])\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        with open(filename) as f:    \n",
    "          data = json.load(f)\n",
    "        print('loading file %s' % (filename))\n",
    "        self.data = data\n",
    "        model_params = np.array(data[0]) # assuming other stuff is in data\n",
    "        self.set_model_params(model_params)\n",
    "\n",
    "    def get_random_model_params(self, stdev=0.1):\n",
    "        return np.random.randn(self.param_count)*stdev\n",
    "\n",
    "    def update_alpha(self):\n",
    "        self.alpha += 0.001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel()\n",
    "model2 = RNNModel2()\n",
    "#model.get_action(np.array([[4]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NPARAMS = model.param_count   # make this a 100-dimensinal problem.\n",
    "NPOPULATION = 410    # use population size of 101.\n",
    "MAX_ITERATION = 4010 # run each solver for 5000 generations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recurrency_label(seq_len):\n",
    "    \n",
    "    labels = []\n",
    "    X = []\n",
    "\n",
    "    X = np.zeros([seq_len,1])\n",
    "    #X[0,:] = 1.0\n",
    "    for ii in range(seq_len):\n",
    "        if ii % 40 == 0:\n",
    "            labels.append(np.ones([1,1]))\n",
    "\n",
    "        else:\n",
    "            labels.append(np.zeros([1,1]))\n",
    "\n",
    "    return X, np.concatenate(labels, axis=0)\n",
    "\n",
    "def evluate_func(data):\n",
    "    model, params =  data\n",
    "    model.set_model_params(params)\n",
    "    model.reset()\n",
    "    loss_cum = 0\n",
    "    Xs, labels  = recurrency_label(42)\n",
    "\n",
    "    for x, label in zip(Xs,labels):\n",
    "        \n",
    "\n",
    "        x = np.array([x])\n",
    "        pred = model.get_action(x)\n",
    "        \n",
    "        #print(label, pred)\n",
    "        loss = BCE_loss(label, pred)\n",
    "        loss_cum += loss\n",
    "    #print(loss_cum)\n",
    "    return -loss_cum\n",
    "\n",
    "def evluate_func(data):\n",
    "    model, params =  data\n",
    "    model.set_model_params(params)\n",
    "    model.reset()\n",
    "    loss_cum = 0\n",
    "    Xs, labels  = recurrency_label(42)\n",
    "\n",
    "    for x, label in zip(Xs,labels):\n",
    "        \n",
    "\n",
    "        x = np.array([x])\n",
    "        pred = model.get_action(x)\n",
    "        \n",
    "        #print(label, pred)\n",
    "        loss = BCE_loss(label, pred)\n",
    "        loss_cum += loss\n",
    "    #print(loss_cum)\n",
    "    return -loss_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defines genetic algorithm solver\n",
    "ga = SimpleGA(NPARAMS,                # number of model parameters\n",
    "               sigma_init=0.5,        # initial standard deviation\n",
    "               popsize=NPOPULATION,   # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               weight_decay=0.00,     # weight decay coefficient\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "fitness at iteration 10 -4.927960193912954\n",
      "fitness at iteration 20 -4.782320077025111\n",
      "fitness at iteration 30 -4.7099182646467685\n",
      "fitness at iteration 40 -4.608970546252618\n",
      "fitness at iteration 50 -4.608970546252618\n",
      "fitness at iteration 60 -4.521226939111405\n",
      "fitness at iteration 70 -4.355134569591871\n",
      "fitness at iteration 80 -4.355134569591871\n",
      "fitness at iteration 90 -3.8315279911265856\n",
      "fitness at iteration 100 -3.8315279911265856\n",
      "fitness at iteration 110 -3.4055754522015484\n",
      "fitness at iteration 120 -3.4055754522015484\n",
      "fitness at iteration 130 -3.4055754522015484\n",
      "fitness at iteration 140 -3.4055754522015484\n",
      "fitness at iteration 150 -3.4055754522015484\n",
      "fitness at iteration 160 -3.223486616896691\n",
      "fitness at iteration 170 -3.223486616896691\n",
      "fitness at iteration 180 -3.2002004968843223\n",
      "fitness at iteration 190 -3.2002004968843223\n",
      "fitness at iteration 200 -3.2002004968843223\n",
      "fitness at iteration 210 -3.2002004968843223\n",
      "fitness at iteration 220 -3.172532842888273\n",
      "fitness at iteration 230 -3.172532842888273\n",
      "fitness at iteration 240 -3.172532842888273\n",
      "fitness at iteration 250 -3.0799588971005125\n",
      "fitness at iteration 260 -2.6467071486099907\n",
      "fitness at iteration 270 -2.6467071486099907\n",
      "fitness at iteration 280 -2.6467071486099907\n",
      "fitness at iteration 290 -2.6467071486099907\n",
      "fitness at iteration 300 -2.6467071486099907\n",
      "fitness at iteration 310 -2.6467071486099907\n",
      "fitness at iteration 320 -2.3816810244484112\n",
      "fitness at iteration 330 -2.3816810244484112\n",
      "fitness at iteration 340 -2.3816810244484112\n",
      "fitness at iteration 350 -2.3816810244484112\n",
      "fitness at iteration 360 -2.3816810244484112\n",
      "fitness at iteration 370 -2.3816810244484112\n",
      "fitness at iteration 380 -2.3816810244484112\n",
      "fitness at iteration 390 -2.3816810244484112\n",
      "fitness at iteration 400 -2.3816810244484112\n",
      "fitness at iteration 410 -2.3816810244484112\n",
      "fitness at iteration 420 -2.3816810244484112\n",
      "fitness at iteration 430 -2.3816810244484112\n",
      "fitness at iteration 440 -2.3816810244484112\n",
      "fitness at iteration 450 -2.3816810244484112\n",
      "fitness at iteration 460 -2.3816810244484112\n",
      "fitness at iteration 470 -2.3816810244484112\n",
      "fitness at iteration 480 -2.3816810244484112\n",
      "fitness at iteration 490 -2.3816810244484112\n",
      "fitness at iteration 500 -2.3816810244484112\n",
      "fitness at iteration 510 -2.3816810244484112\n",
      "fitness at iteration 520 -2.3816810244484112\n",
      "fitness at iteration 530 -2.3816810244484112\n",
      "fitness at iteration 540 -2.3816810244484112\n",
      "fitness at iteration 550 -2.3816810244484112\n",
      "fitness at iteration 560 -2.3816810244484112\n",
      "fitness at iteration 570 -2.3816810244484112\n",
      "fitness at iteration 580 -2.3816810244484112\n",
      "fitness at iteration 590 -2.3816810244484112\n",
      "fitness at iteration 600 -2.3816810244484112\n",
      "fitness at iteration 610 -2.3816810244484112\n",
      "fitness at iteration 620 -2.3816810244484112\n",
      "fitness at iteration 630 -2.329576872307366\n",
      "fitness at iteration 640 -2.329576872307366\n",
      "fitness at iteration 650 -2.329576872307366\n",
      "fitness at iteration 660 -2.329576872307366\n",
      "fitness at iteration 670 -2.329576872307366\n",
      "fitness at iteration 680 -2.329576872307366\n",
      "fitness at iteration 690 -1.7192341300496103\n",
      "fitness at iteration 700 -1.7192341300496103\n",
      "fitness at iteration 710 -1.7192341300496103\n",
      "fitness at iteration 720 -1.7192341300496103\n",
      "fitness at iteration 730 -1.7192341300496103\n",
      "fitness at iteration 740 -1.7192341300496103\n",
      "fitness at iteration 750 -1.7192341300496103\n",
      "fitness at iteration 760 -1.7192341300496103\n",
      "fitness at iteration 770 -1.7192341300496103\n",
      "fitness at iteration 780 -1.7192341300496103\n",
      "fitness at iteration 790 -1.3025995453218466\n",
      "fitness at iteration 800 -1.3025995453218466\n",
      "fitness at iteration 810 -1.3025995453218466\n",
      "fitness at iteration 820 -1.3025995453218466\n",
      "fitness at iteration 830 -1.3025995453218466\n",
      "fitness at iteration 840 -1.3025995453218466\n",
      "fitness at iteration 850 -1.3025995453218466\n",
      "fitness at iteration 860 -1.3025995453218466\n",
      "fitness at iteration 870 -1.3025995453218466\n",
      "fitness at iteration 880 -1.2203966608166377\n",
      "fitness at iteration 890 -1.2203966608166377\n",
      "fitness at iteration 900 -1.2203966608166377\n",
      "fitness at iteration 910 -1.0763636040131548\n",
      "fitness at iteration 920 -1.0198808447740497\n",
      "fitness at iteration 930 -1.0198808447740497\n",
      "fitness at iteration 940 -0.820934520978588\n",
      "fitness at iteration 950 -0.5734600957804917\n",
      "fitness at iteration 960 -0.5734600957804917\n",
      "fitness at iteration 970 -0.5734600957804917\n",
      "fitness at iteration 980 -0.5734600957804917\n",
      "fitness at iteration 990 -0.5734600957804917\n",
      "fitness at iteration 1000 -0.5734600957804917\n",
      "fitness at iteration 1010 -0.5734600957804917\n",
      "fitness at iteration 1020 -0.5734600957804917\n",
      "fitness at iteration 1030 -0.37172113466290446\n",
      "fitness at iteration 1040 -0.37172113466290446\n",
      "fitness at iteration 1050 -0.37172113466290446\n",
      "fitness at iteration 1060 -0.3637248668760548\n",
      "fitness at iteration 1070 -0.3637248668760548\n",
      "fitness at iteration 1080 -0.3637248668760548\n",
      "fitness at iteration 1090 -0.3637248668760548\n",
      "fitness at iteration 1100 -0.3637248668760548\n",
      "fitness at iteration 1110 -0.3637248668760548\n",
      "fitness at iteration 1120 -0.3637248668760548\n",
      "fitness at iteration 1130 -0.3637248668760548\n",
      "fitness at iteration 1140 -0.3637248668760548\n",
      "fitness at iteration 1150 -0.3637248668760548\n",
      "fitness at iteration 1160 -0.3637248668760548\n",
      "fitness at iteration 1170 -0.3637248668760548\n",
      "fitness at iteration 1180 -0.3637248668760548\n",
      "fitness at iteration 1190 -0.3637248668760548\n",
      "fitness at iteration 1200 -0.3637248668760548\n",
      "fitness at iteration 1210 -0.3637248668760548\n",
      "fitness at iteration 1220 -0.3637248668760548\n",
      "fitness at iteration 1230 -0.3637248668760548\n",
      "fitness at iteration 1240 -0.3637248668760548\n",
      "fitness at iteration 1250 -0.3637248668760548\n",
      "fitness at iteration 1260 -0.3637248668760548\n",
      "fitness at iteration 1270 -0.3637248668760548\n",
      "fitness at iteration 1280 -0.3637248668760548\n",
      "fitness at iteration 1290 -0.3637248668760548\n",
      "fitness at iteration 1300 -0.3637248668760548\n",
      "fitness at iteration 1310 -0.3637248668760548\n",
      "fitness at iteration 1320 -0.3637248668760548\n",
      "fitness at iteration 1330 -0.3637248668760548\n",
      "fitness at iteration 1340 -0.3637248668760548\n",
      "fitness at iteration 1350 -0.3637248668760548\n",
      "fitness at iteration 1360 -0.3571167191938542\n",
      "fitness at iteration 1370 -0.3571167191938542\n",
      "fitness at iteration 1380 -0.3571167191938542\n",
      "fitness at iteration 1390 -0.3571167191938542\n",
      "fitness at iteration 1400 -0.3571167191938542\n",
      "fitness at iteration 1410 -0.3190360057173094\n",
      "fitness at iteration 1420 -0.3190360057173094\n",
      "fitness at iteration 1430 -0.3190360057173094\n",
      "fitness at iteration 1440 -0.3190360057173094\n",
      "fitness at iteration 1450 -0.3190360057173094\n",
      "fitness at iteration 1460 -0.3190360057173094\n",
      "fitness at iteration 1470 -0.3190360057173094\n",
      "fitness at iteration 1480 -0.2737743596089571\n",
      "fitness at iteration 1490 -0.22496452990526347\n",
      "fitness at iteration 1500 -0.22496452990526347\n",
      "fitness at iteration 1510 -0.22496452990526347\n",
      "fitness at iteration 1520 -0.22496452990526347\n",
      "fitness at iteration 1530 -0.22496452990526347\n",
      "fitness at iteration 1540 -0.22496452990526347\n",
      "fitness at iteration 1550 -0.22496452990526347\n",
      "fitness at iteration 1560 -0.22490813807508483\n",
      "fitness at iteration 1570 -0.22490813807508483\n",
      "fitness at iteration 1580 -0.17384325580792076\n",
      "fitness at iteration 1590 -0.17384325580792076\n",
      "fitness at iteration 1600 -0.17384325580792076\n",
      "fitness at iteration 1610 -0.17384325580792076\n",
      "fitness at iteration 1620 -0.17384325580792076\n",
      "fitness at iteration 1630 -0.17384325580792076\n",
      "fitness at iteration 1640 -0.17384325580792076\n",
      "fitness at iteration 1650 -0.17384325580792076\n",
      "fitness at iteration 1660 -0.17384325580792076\n",
      "fitness at iteration 1670 -0.17384325580792076\n",
      "fitness at iteration 1680 -0.17384325580792076\n",
      "fitness at iteration 1690 -0.17384325580792076\n",
      "fitness at iteration 1700 -0.17384325580792076\n",
      "fitness at iteration 1710 -0.17384325580792076\n",
      "fitness at iteration 1720 -0.17384325580792076\n",
      "fitness at iteration 1730 -0.17384325580792076\n",
      "fitness at iteration 1740 -0.17384325580792076\n",
      "fitness at iteration 1750 -0.17384325580792076\n",
      "fitness at iteration 1760 -0.17384325580792076\n",
      "fitness at iteration 1770 -0.17384325580792076\n",
      "fitness at iteration 1780 -0.17384325580792076\n",
      "fitness at iteration 1790 -0.17384325580792076\n",
      "fitness at iteration 1800 -0.16955981124338704\n",
      "fitness at iteration 1810 -0.16659693931416025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness at iteration 1820 -0.16659693931416025\n",
      "fitness at iteration 1830 -0.16659693931416025\n",
      "fitness at iteration 1840 -0.16659693931416025\n",
      "fitness at iteration 1850 -0.16659693931416025\n",
      "fitness at iteration 1860 -0.11305249763125212\n",
      "fitness at iteration 1870 -0.11305249763125212\n",
      "fitness at iteration 1880 -0.11305249763125212\n",
      "fitness at iteration 1890 -0.11305249763125212\n",
      "fitness at iteration 1900 -0.11305249763125212\n",
      "fitness at iteration 1910 -0.11305249763125212\n",
      "fitness at iteration 1920 -0.11305249763125212\n",
      "fitness at iteration 1930 -0.08393007679591097\n",
      "fitness at iteration 1940 -0.08393007679591097\n",
      "fitness at iteration 1950 -0.08393007679591097\n",
      "fitness at iteration 1960 -0.08393007679591097\n",
      "fitness at iteration 1970 -0.08393007679591097\n",
      "fitness at iteration 1980 -0.08393007679591097\n",
      "fitness at iteration 1990 -0.072642038029248\n",
      "fitness at iteration 2000 -0.072642038029248\n",
      "fitness at iteration 2010 -0.072642038029248\n",
      "fitness at iteration 2020 -0.072642038029248\n",
      "fitness at iteration 2030 -0.072642038029248\n",
      "fitness at iteration 2040 -0.072642038029248\n",
      "fitness at iteration 2050 -0.072642038029248\n",
      "fitness at iteration 2060 -0.04814051665065966\n",
      "fitness at iteration 2070 -0.04814051665065966\n",
      "fitness at iteration 2080 -0.04814051665065966\n",
      "fitness at iteration 2090 -0.04814051665065966\n",
      "fitness at iteration 2100 -0.04548203700714902\n",
      "fitness at iteration 2110 -0.04548203700714902\n",
      "fitness at iteration 2120 -0.04548203700714902\n",
      "fitness at iteration 2130 -0.04548203700714902\n",
      "fitness at iteration 2140 -0.03777274739039381\n",
      "fitness at iteration 2150 -0.03777274739039381\n",
      "fitness at iteration 2160 -0.03777274739039381\n",
      "fitness at iteration 2170 -0.03777274739039381\n",
      "fitness at iteration 2180 -0.03777274739039381\n",
      "fitness at iteration 2190 -0.03719802546102488\n",
      "fitness at iteration 2200 -0.03719802546102488\n",
      "fitness at iteration 2210 -0.033395586823338284\n",
      "fitness at iteration 2220 -0.033395586823338284\n",
      "fitness at iteration 2230 -0.033395586823338284\n",
      "fitness at iteration 2240 -0.033395586823338284\n",
      "fitness at iteration 2250 -0.033395586823338284\n",
      "fitness at iteration 2260 -0.02999868708704341\n",
      "fitness at iteration 2270 -0.028654574892393654\n",
      "fitness at iteration 2280 -0.028654574892393654\n",
      "fitness at iteration 2290 -0.028654574892393654\n",
      "fitness at iteration 2300 -0.028654574892393654\n",
      "fitness at iteration 2310 -0.028654574892393654\n",
      "fitness at iteration 2320 -0.028654574892393654\n",
      "fitness at iteration 2330 -0.028654574892393654\n",
      "fitness at iteration 2340 -0.02731969100024502\n",
      "fitness at iteration 2350 -0.0178651090038848\n",
      "fitness at iteration 2360 -0.0178651090038848\n",
      "fitness at iteration 2370 -0.0178651090038848\n",
      "fitness at iteration 2380 -0.0178651090038848\n",
      "fitness at iteration 2390 -0.0178651090038848\n",
      "fitness at iteration 2400 -0.0178651090038848\n",
      "fitness at iteration 2410 -0.0178651090038848\n",
      "fitness at iteration 2420 -0.0178651090038848\n",
      "fitness at iteration 2430 -0.0178651090038848\n",
      "fitness at iteration 2440 -0.0178651090038848\n",
      "fitness at iteration 2450 -0.0178651090038848\n",
      "fitness at iteration 2460 -0.0178651090038848\n",
      "fitness at iteration 2470 -0.0178651090038848\n",
      "fitness at iteration 2480 -0.0178651090038848\n",
      "fitness at iteration 2490 -0.0178651090038848\n",
      "fitness at iteration 2500 -0.0178651090038848\n",
      "fitness at iteration 2510 -0.0178651090038848\n",
      "fitness at iteration 2520 -0.0178651090038848\n",
      "fitness at iteration 2530 -0.0178651090038848\n",
      "fitness at iteration 2540 -0.0178651090038848\n",
      "fitness at iteration 2550 -0.0178651090038848\n",
      "fitness at iteration 2560 -0.0178651090038848\n",
      "fitness at iteration 2570 -0.016573921000249826\n",
      "fitness at iteration 2580 -0.016573921000249826\n",
      "fitness at iteration 2590 -0.016573921000249826\n",
      "fitness at iteration 2600 -0.016573921000249826\n",
      "fitness at iteration 2610 -0.016573921000249826\n",
      "fitness at iteration 2620 -0.016573921000249826\n",
      "fitness at iteration 2630 -0.01598093039869993\n",
      "fitness at iteration 2640 -0.01598093039869993\n",
      "fitness at iteration 2650 -0.01598093039869993\n",
      "fitness at iteration 2660 -0.01598093039869993\n",
      "fitness at iteration 2670 -0.01598093039869993\n",
      "fitness at iteration 2680 -0.01598093039869993\n",
      "fitness at iteration 2690 -0.012177374439293863\n",
      "fitness at iteration 2700 -0.012177374439293863\n",
      "fitness at iteration 2710 -0.012177374439293863\n",
      "fitness at iteration 2720 -0.012177374439293863\n",
      "fitness at iteration 2730 -0.012177374439293863\n",
      "fitness at iteration 2740 -0.012177374439293863\n",
      "fitness at iteration 2750 -0.012177374439293863\n",
      "fitness at iteration 2760 -0.012177374439293863\n",
      "fitness at iteration 2770 -0.012177374439293863\n",
      "fitness at iteration 2780 -0.012177374439293863\n",
      "fitness at iteration 2790 -0.012177374439293863\n",
      "fitness at iteration 2800 -0.012177374439293863\n",
      "fitness at iteration 2810 -0.012177374439293863\n",
      "fitness at iteration 2820 -0.012177374439293863\n",
      "fitness at iteration 2830 -0.011223760828894942\n",
      "fitness at iteration 2840 -0.011223760828894942\n",
      "fitness at iteration 2850 -0.011223760828894942\n",
      "fitness at iteration 2860 -0.011193048677952297\n",
      "fitness at iteration 2870 -0.009184864284526244\n",
      "fitness at iteration 2880 -0.007826525954398093\n",
      "fitness at iteration 2890 -0.007826525954398093\n",
      "fitness at iteration 2900 -0.007826525954398093\n",
      "fitness at iteration 2910 -0.007826525954398093\n",
      "fitness at iteration 2920 -0.007386964626755582\n",
      "fitness at iteration 2930 -0.007386964626755582\n",
      "fitness at iteration 2940 -0.007386964626755582\n",
      "fitness at iteration 2950 -0.007386964626755582\n",
      "fitness at iteration 2960 -0.007386964626755582\n",
      "fitness at iteration 2970 -0.007386964626755582\n",
      "fitness at iteration 2980 -0.007386964626755582\n",
      "fitness at iteration 2990 -0.007386964626755582\n",
      "fitness at iteration 3000 -0.007386964626755582\n",
      "fitness at iteration 3010 -0.007386964626755582\n",
      "fitness at iteration 3020 -0.007241361965102591\n",
      "fitness at iteration 3030 -0.007241361965102591\n",
      "fitness at iteration 3040 -0.004873748495240058\n",
      "fitness at iteration 3050 -0.004873748495240058\n",
      "fitness at iteration 3060 -0.004873748495240058\n",
      "fitness at iteration 3070 -0.004873748495240058\n",
      "fitness at iteration 3080 -0.004873748495240058\n",
      "fitness at iteration 3090 -0.004873748495240058\n",
      "fitness at iteration 3100 -0.004873748495240058\n",
      "fitness at iteration 3110 -0.004873748495240058\n",
      "fitness at iteration 3120 -0.004873748495240058\n",
      "fitness at iteration 3130 -0.004873748495240058\n",
      "fitness at iteration 3140 -0.004873748495240058\n",
      "fitness at iteration 3150 -0.004873748495240058\n",
      "fitness at iteration 3160 -0.004873748495240058\n",
      "fitness at iteration 3170 -0.004873748495240058\n",
      "fitness at iteration 3180 -0.004873748495240058\n",
      "fitness at iteration 3190 -0.004873748495240058\n",
      "fitness at iteration 3200 -0.004873748495240058\n",
      "fitness at iteration 3210 -0.004873748495240058\n",
      "fitness at iteration 3220 -0.004381496618795574\n",
      "fitness at iteration 3230 -0.0033456509928772647\n",
      "fitness at iteration 3240 -0.0033456509928772647\n",
      "fitness at iteration 3250 -0.0033456509928772647\n",
      "fitness at iteration 3260 -0.0033456509928772647\n",
      "fitness at iteration 3270 -0.0033456509928772647\n",
      "fitness at iteration 3280 -0.0033456509928772647\n",
      "fitness at iteration 3290 -0.0032304650920535224\n",
      "fitness at iteration 3300 -0.0032304650920535224\n",
      "fitness at iteration 3310 -0.0032304650920535224\n",
      "fitness at iteration 3320 -0.0032304650920535224\n",
      "fitness at iteration 3330 -0.0032304650920535224\n",
      "fitness at iteration 3340 -0.0032304650920535224\n",
      "fitness at iteration 3350 -0.0032304650920535224\n",
      "fitness at iteration 3360 -0.0032304650920535224\n",
      "fitness at iteration 3370 -0.0032304650920535224\n",
      "fitness at iteration 3380 -0.0032304650920535224\n",
      "fitness at iteration 3390 -0.0032304650920535224\n",
      "fitness at iteration 3400 -0.0032304650920535224\n",
      "fitness at iteration 3410 -0.0032304650920535224\n",
      "fitness at iteration 3420 -0.0032304650920535224\n",
      "fitness at iteration 3430 -0.0032304650920535224\n",
      "fitness at iteration 3440 -0.0032304650920535224\n",
      "fitness at iteration 3450 -0.0026600710814317593\n",
      "fitness at iteration 3460 -0.0026600710814317593\n",
      "fitness at iteration 3470 -0.0026600710814317593\n",
      "fitness at iteration 3480 -0.0026600710814317593\n",
      "fitness at iteration 3490 -0.0026600710814317593\n",
      "fitness at iteration 3500 -0.0026600710814317593\n",
      "fitness at iteration 3510 -0.0026600710814317593\n",
      "fitness at iteration 3520 -0.0025837346067082677\n",
      "fitness at iteration 3530 -0.0025837346067082677\n",
      "fitness at iteration 3540 -0.0025837346067082677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness at iteration 3550 -0.0025837346067082677\n",
      "fitness at iteration 3560 -0.0025837346067082677\n",
      "fitness at iteration 3570 -0.0024510775820761397\n",
      "fitness at iteration 3580 -0.0024510775820761397\n",
      "fitness at iteration 3590 -0.0021326943574112643\n",
      "fitness at iteration 3600 -0.0021326943574112643\n",
      "fitness at iteration 3610 -0.0021326943574112643\n",
      "fitness at iteration 3620 -0.0021326943574112643\n",
      "fitness at iteration 3630 -0.0021326943574112643\n",
      "fitness at iteration 3640 -0.0021326943574112643\n",
      "fitness at iteration 3650 -0.0021326943574112643\n",
      "fitness at iteration 3660 -0.0021326943574112643\n",
      "fitness at iteration 3670 -0.0021326943574112643\n",
      "fitness at iteration 3680 -0.002081654536214025\n",
      "fitness at iteration 3690 -0.0020255376142373615\n",
      "fitness at iteration 3700 -0.001947188012600975\n",
      "fitness at iteration 3710 -0.001947188012600975\n",
      "fitness at iteration 3720 -0.0018646307555521117\n",
      "fitness at iteration 3730 -0.0018192269069762222\n",
      "fitness at iteration 3740 -0.0018192269069762222\n",
      "fitness at iteration 3750 -0.0018117588274780213\n",
      "fitness at iteration 3760 -0.0018117588274780213\n",
      "fitness at iteration 3770 -0.0018117588274780213\n",
      "fitness at iteration 3780 -0.0018117588274780213\n",
      "fitness at iteration 3790 -0.0015677580060210275\n",
      "fitness at iteration 3800 -0.0015677580060210275\n",
      "fitness at iteration 3810 -0.0015677580060210275\n",
      "fitness at iteration 3820 -0.0015677580060210275\n",
      "fitness at iteration 3830 -0.0015677580060210275\n",
      "fitness at iteration 3840 -0.0015677580060210275\n",
      "fitness at iteration 3850 -0.0015153340100984502\n",
      "fitness at iteration 3860 -0.0015153340100984502\n",
      "fitness at iteration 3870 -0.0015153340100984502\n",
      "fitness at iteration 3880 -0.0014791044234245447\n",
      "fitness at iteration 3890 -0.0014791044234245447\n",
      "fitness at iteration 3900 -0.0014791044234245447\n",
      "fitness at iteration 3910 -0.0014791044234245447\n",
      "fitness at iteration 3920 -0.0014791044234245447\n",
      "fitness at iteration 3930 -0.0014791044234245447\n",
      "fitness at iteration 3940 -0.0014791044234245447\n",
      "fitness at iteration 3950 -0.0014791044234245447\n",
      "fitness at iteration 3960 -0.0014791044234245447\n",
      "fitness at iteration 3970 -0.0014468121976937457\n",
      "fitness at iteration 3980 -0.0014468121976937457\n",
      "fitness at iteration 3990 -0.0013266921248694362\n",
      "fitness at iteration 4000 -0.0013266921248694362\n",
      "fitness at iteration 4010 -0.0013266921248694362\n",
      "fitness at iteration 4020 -0.0013266921248694362\n",
      "fitness at iteration 4030 -0.0013266921248694362\n",
      "fitness at iteration 4040 -0.0013266921248694362\n",
      "fitness at iteration 4050 -0.0013266921248694362\n",
      "fitness at iteration 4060 -0.0012758121803552174\n",
      "fitness at iteration 4070 -0.0012758121803552174\n",
      "fitness at iteration 4080 -0.0012758121803552174\n",
      "fitness at iteration 4090 -0.0012758121803552174\n",
      "fitness at iteration 4100 -0.0012758121803552174\n",
      "fitness at iteration 4110 -0.0012758121803552174\n",
      "fitness at iteration 4120 -0.0012758121803552174\n",
      "fitness at iteration 4130 -0.0012758121803552174\n",
      "fitness at iteration 4140 -0.0012758121803552174\n",
      "fitness at iteration 4150 -0.0012758121803552174\n",
      "fitness at iteration 4160 -0.0012552044868378834\n",
      "fitness at iteration 4170 -0.0012063505300118748\n",
      "fitness at iteration 4180 -0.0012063505300118748\n",
      "fitness at iteration 4190 -0.0011982708357405486\n",
      "fitness at iteration 4200 -0.0011493159367627953\n",
      "fitness at iteration 4210 -0.0011493159367627953\n",
      "fitness at iteration 4220 -0.0011410020972522088\n",
      "fitness at iteration 4230 -0.001122817282059863\n",
      "fitness at iteration 4240 -0.0011224258774387003\n",
      "fitness at iteration 4250 -0.0011224258774387003\n",
      "fitness at iteration 4260 -0.0011224258774387003\n",
      "fitness at iteration 4270 -0.0011061028815362603\n",
      "fitness at iteration 4280 -0.0011061028815362603\n",
      "fitness at iteration 4290 -0.0011061028815362603\n",
      "fitness at iteration 4300 -0.0011061028815362603\n",
      "fitness at iteration 4310 -0.0010974607640493713\n",
      "fitness at iteration 4320 -0.0010974607640493713\n",
      "fitness at iteration 4330 -0.0010974607640493713\n",
      "fitness at iteration 4340 -0.0010583482904784837\n",
      "fitness at iteration 4350 -0.001044456340512435\n",
      "fitness at iteration 4360 -0.0010201297902655549\n",
      "fitness at iteration 4370 -0.0010201297902655549\n",
      "fitness at iteration 4380 -0.0010201297902655549\n",
      "fitness at iteration 4390 -0.0010201297902655549\n",
      "fitness at iteration 4400 -0.0009891991148420149\n",
      "fitness at iteration 4410 -0.0009891991148420149\n",
      "fitness at iteration 4420 -0.0009891991148420149\n",
      "fitness at iteration 4430 -0.0009891991148420149\n",
      "fitness at iteration 4440 -0.0009891991148420149\n",
      "fitness at iteration 4450 -0.0009891991148420149\n",
      "fitness at iteration 4460 -0.000968651838574464\n",
      "fitness at iteration 4470 -0.000968651838574464\n",
      "fitness at iteration 4480 -0.000968651838574464\n",
      "fitness at iteration 4490 -0.0009667633741869719\n",
      "fitness at iteration 4500 -0.0009667633741869719\n",
      "fitness at iteration 4510 -0.0009667633741869719\n",
      "fitness at iteration 4520 -0.0009667633741869719\n",
      "fitness at iteration 4530 -0.0009610356453579281\n",
      "fitness at iteration 4540 -0.000941554208690999\n",
      "fitness at iteration 4550 -0.000941554208690999\n",
      "fitness at iteration 4560 -0.0009299114209642946\n",
      "fitness at iteration 4570 -0.0009299114209642946\n",
      "fitness at iteration 4580 -0.0009195389386280503\n",
      "fitness at iteration 4590 -0.0008865637686091368\n",
      "fitness at iteration 4600 -0.0008865637686091368\n",
      "fitness at iteration 4610 -0.0008770936220638961\n",
      "fitness at iteration 4620 -0.0008770936220638961\n",
      "fitness at iteration 4630 -0.0008631362707786664\n",
      "fitness at iteration 4640 -0.0008631362707786664\n",
      "fitness at iteration 4650 -0.0008631362707786664\n",
      "fitness at iteration 4660 -0.0008631362707786664\n",
      "fitness at iteration 4670 -0.0008631362707786664\n",
      "fitness at iteration 4680 -0.0008514215998840732\n",
      "fitness at iteration 4690 -0.0008514215998840732\n",
      "fitness at iteration 4700 -0.0008514215998840732\n",
      "fitness at iteration 4710 -0.0008514215998840732\n",
      "fitness at iteration 4720 -0.0008514215998840732\n",
      "fitness at iteration 4730 -0.0008514215998840732\n",
      "fitness at iteration 4740 -0.0008299840161074744\n",
      "fitness at iteration 4750 -0.0008299840161074744\n",
      "fitness at iteration 4760 -0.0008299840161074744\n",
      "fitness at iteration 4770 -0.0008167934752082498\n",
      "fitness at iteration 4780 -0.0008167934752082498\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(mp.cpu_count())\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "fit_func = evluate_func\n",
    "# defines a function to use solver to solve fit_func\n",
    "def test_solver(solver):\n",
    "    history = []\n",
    "    j = 0\n",
    "    while True:\n",
    "        solutions = solver.ask()\n",
    "        fitness_list = np.zeros(solver.popsize)\n",
    "        #print(solutions)\n",
    "        fitness_list = pool.map(fit_func, [(model,solutions[i]) for i in range(solver.popsize)])\n",
    "            \n",
    "\n",
    "        solver.tell(fitness_list)\n",
    "        result = solver.result() # first element is the best solution, second element is the best fitness\n",
    "        history.append(result[1])\n",
    "        if (j+1) % 10 == 0:\n",
    "            print(\"fitness at iteration\", (j+1), result[1])\n",
    "#             print(\"Best:\",solver.elite_rewards[0])   \n",
    "            \n",
    "#         if -solver.elite_rewards[0] < 1:\n",
    "#             model.update_alpha()\n",
    "#             print('Alpha changed to', model.alpha)\n",
    "#             for kk in range(len(solver.elite_rewards)):\n",
    "#                 solver.elite_rewards[kk] = fit_func((model,solver.elite_params[kk]))\n",
    "#                 solver.forget_best = True\n",
    "#             else:\n",
    "#                 solver.forget_best = False\n",
    "        if -result[1] <= 0.0001:\n",
    "            print(\"local optimum discovered by solver:\\n\", result[0])\n",
    "            print(\"fitness score at this local optimum:\", result[1])\n",
    "            return history, result\n",
    "        j += 1\n",
    "        \n",
    "             \n",
    "    \n",
    "\n",
    "ga_history, result = test_solver(ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "fitness at iteration 10 -5.279535869448046\n",
      "fitness at iteration 20 -5.279535869448046\n",
      "fitness at iteration 30 -5.279535869448046\n",
      "fitness at iteration 40 -5.279535869448046\n",
      "fitness at iteration 50 -5.279535869448046\n",
      "fitness at iteration 60 -5.279535869448046\n",
      "fitness at iteration 70 -5.279535869448046\n",
      "fitness at iteration 80 -5.279535869448046\n",
      "fitness at iteration 90 -5.279535869448046\n",
      "fitness at iteration 100 -5.279535869448046\n",
      "fitness at iteration 110 -5.279535869448046\n",
      "fitness at iteration 120 -5.279535869448046\n",
      "fitness at iteration 130 -5.279535869448046\n",
      "fitness at iteration 140 -5.279535869448046\n",
      "fitness at iteration 150 -5.279535869448046\n",
      "fitness at iteration 160 -5.279535869448046\n",
      "fitness at iteration 170 -5.279535869448046\n",
      "fitness at iteration 180 -5.279535869448046\n",
      "fitness at iteration 190 -5.279535869448046\n",
      "fitness at iteration 200 -5.279535869448046\n",
      "fitness at iteration 210 -5.279535869448046\n",
      "fitness at iteration 220 -5.279535869448046\n",
      "fitness at iteration 230 -5.279535869448046\n",
      "fitness at iteration 240 -5.279535869448046\n",
      "fitness at iteration 250 -5.279535869448046\n",
      "fitness at iteration 260 -5.279535869448046\n",
      "fitness at iteration 270 -5.279535869448046\n",
      "fitness at iteration 280 -5.279535869448046\n",
      "fitness at iteration 290 -5.279535869448046\n",
      "fitness at iteration 300 -5.279535869448046\n",
      "fitness at iteration 310 -5.279535869448046\n",
      "fitness at iteration 320 -5.279535869448046\n",
      "fitness at iteration 330 -5.279535869448046\n",
      "fitness at iteration 340 -5.279535869448046\n",
      "fitness at iteration 350 -5.279535869448046\n",
      "fitness at iteration 360 -5.279535869448046\n",
      "fitness at iteration 370 -5.279535869448046\n",
      "fitness at iteration 380 -5.279535869448046\n",
      "fitness at iteration 390 -5.279535869448046\n",
      "fitness at iteration 400 -5.279535869448046\n",
      "fitness at iteration 410 -5.279535869448046\n",
      "fitness at iteration 420 -5.279535869448046\n",
      "fitness at iteration 430 -5.279535869448046\n",
      "fitness at iteration 440 -5.279535869448046\n",
      "fitness at iteration 450 -5.279535869448046\n",
      "fitness at iteration 460 -5.279535869448046\n"
     ]
    }
   ],
   "source": [
    "def MSE_loss(y,p):\n",
    "    \n",
    "    return np.mean(np.abs(y - p))\n",
    "\n",
    "\n",
    "def evluate_func2(data):\n",
    "    model, target_model, params =  data\n",
    "    model.set_model_params(params)\n",
    "    model.reset()\n",
    "    target_model.reset()\n",
    "    loss_cum = 0\n",
    "    Xs, labels  = recurrency_label(42)\n",
    "\n",
    "    for x, label in zip(Xs,labels):\n",
    "        \n",
    "\n",
    "        x = np.array([x])\n",
    "        ac = target_model.get_action(x)\n",
    "        pred = model.get_action(x)\n",
    "        \n",
    "        \n",
    "        #print(label, pred)\n",
    "        loss = MSE_loss(target_model.h, pred)\n",
    "#         print(target_model.h, pred)\n",
    "        loss_cum += loss\n",
    "    #print(loss_cum)\n",
    "    return -loss_cum\n",
    "\n",
    "ga = SimpleGA(model2.param_count,                # number of model parameters\n",
    "               sigma_init=0.5,        # initial standard deviation\n",
    "               popsize=NPOPULATION,   # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               weight_decay=0.00,     # weight decay coefficient\n",
    "              )\n",
    "\n",
    "\n",
    "print(mp.cpu_count())\n",
    "\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "fit_func = evluate_func2\n",
    "# defines a function to use solver to solve fit_func\n",
    "def test_solver(solver, target_model):\n",
    "    history = []\n",
    "    j = 0\n",
    "    while True:\n",
    "        solutions = solver.ask()\n",
    "        fitness_list = np.zeros(solver.popsize)\n",
    "\n",
    "        fitness_list = pool.map(fit_func, [(model2, target_model, solutions[i]) for i in range(solver.popsize)])\n",
    "            \n",
    "\n",
    "        solver.tell(fitness_list)\n",
    "        result = solver.result() # first element is the best solution, second element is the best fitness\n",
    "        history.append(result[1])\n",
    "        if (j+1) % 10 == 0:\n",
    "            print(\"fitness at iteration\", (j+1), result[1])\n",
    "        \n",
    "\n",
    "        if result[1] >= 0.0001:\n",
    "            break\n",
    "        #j += 1\n",
    "        \n",
    "             \n",
    "    print(\"local optimum discovered by solver:\\n\", result[0])\n",
    "    print(\"fitness score at this local optimum:\", result[1])\n",
    "    return history, result\n",
    "\n",
    "model.set_model_params(result[0])\n",
    "target_model = model\n",
    "#print(target_model)\n",
    "ga_history, result = test_solver(ga, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ga.elite_params)\n",
    "ga.elite_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6042791e59ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(loss_cum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss_cum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mevluate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def evluate_func(model, target_model, params):\n",
    "#     model, target_model, params =  data\n",
    "    model.set_model_params(params)\n",
    "    model.reset()\n",
    "    target_model.reset()\n",
    "    loss_cum = 0\n",
    "    Xs, labels  = recurrency_label(42)\n",
    "\n",
    "    for x, label in zip(Xs,labels):\n",
    "        \n",
    "\n",
    "        x = np.array([x])\n",
    "        ac = target_model.get_action(x)\n",
    "        pred = model.get_action(x)\n",
    "        \n",
    "        #print(binarize(model.h))\n",
    "        \n",
    "        #print(label, pred)\n",
    "        loss = BCE_loss(target_model.h, pred)\n",
    "        print(target_model.h[0,3],pred[6])\n",
    "        loss_cum += loss\n",
    "    #print(loss_cum)\n",
    "    return -loss_cum\n",
    "evluate_func(model2,target_model, result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaes = CMAES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              weight_decay=0.0,\n",
    "              sigma_init = 2.0\n",
    "          )\n",
    "cma_history = test_solver(cmaes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines PEPG (NES) solver\n",
    "pepg = PEPG(NPARAMS,                         # number of model parameters\n",
    "            sigma_init=0.5,                  # initial standard deviation\n",
    "            learning_rate=0.01,               # learning rate for standard deviation\n",
    "            learning_rate_decay=1.0,       # don't anneal the learning rate\n",
    "            popsize=NPOPULATION,             # population size\n",
    "            average_baseline=False,          # set baseline to average of batch\n",
    "            weight_decay=0.00,            # weight decay coefficient\n",
    "            rank_fitness=False,           # use rank rather than fitness numbers\n",
    "            forget_best=False)   \n",
    "pepg_history = test_solver(pepg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = ga.ask()\n",
    "evluate_func(model, solutions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model.forward(np.ones([1,12]))\n",
    "#model.forward(X[0])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([20, 12]), array([20, 20]), array([11, 20])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lyr_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ARC]",
   "language": "python",
   "name": "conda-env-ARC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
